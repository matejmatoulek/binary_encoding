{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://github.com/lukas/ml-class/blob/master/videos/text-gen/char-gen.py\n",
    "# but with binary encoding of characters\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config['batch_size'] = 256\n",
    "config['file'] = 'english_names.csv'\n",
    "# length of window of tokens preceding predicted token\n",
    "config['maxlen'] = 3\n",
    "config['epochs'] = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_prepared_data(sequence_list):\n",
    "    max_tokens = max(map(len, sequence_list))\n",
    "    sequences_tokens_np = pad_sequences(sequence_list, maxlen=max_tokens, dtype=object, padding='post', value='')\n",
    "    tokens_vocab = list(np.unique(sequences_tokens_np))\n",
    "    tokens_to_idx = {token:idx for idx, token in enumerate(tokens_vocab)}\n",
    "    idx_to_tokens = {idx:token for idx, token in enumerate(tokens_vocab)}\n",
    "    \n",
    "    return {\n",
    "        # number of unique tokens\n",
    "        'vocab_size': len(tokens_vocab),\n",
    "        # maximum length of sequence\n",
    "        'max_tokens': max_tokens,\n",
    "        # sequences splited into separate tokens as numpy array\n",
    "        'sequences_tokens_np': sequences_tokens_np,\n",
    "        # list of tokens present in corpus\n",
    "        'tokens_vocab': tokens_vocab,\n",
    "        # mapping of tokens to integer identifiers\n",
    "        'tokens_to_idx': tokens_to_idx,\n",
    "        # mapping of integer identifiers to tokens\n",
    "        'idx_to_tokens': idx_to_tokens\n",
    "    }\n",
    "\n",
    "# encodes sequences of tokens to their integer identifiers using the mapping\n",
    "def encode_tokens_to_idx(sequences_tokens_np, tokens_to_idx):\n",
    "    return np.vectorize(tokens_to_idx.get)(sequences_tokens_np)\n",
    "\n",
    "# encode array of integer sequences (category ids) to binaray encoding (5 = 101)\n",
    "# this is an alternative to one-hot-encoding\n",
    "def binary_system_encode(idx_np_array, num_bits):\n",
    "    # https://stackoverflow.com/a/22227898/9123190\n",
    "    return (((idx_np_array[:,:,None] & (1 << np.arange(num_bits)))) > 0).astype(int)\n",
    "\n",
    "# reverse opeation to binary encoding. from binary encoded sequences to category indentifiers\n",
    "def binary_system_decode(binary_encoded_array):\n",
    "    # https://stackoverflow.com/a/15506055/9123190\n",
    "    return binary_encoded_array.dot(1 << np.arange(binary_encoded_array.shape[-1]))\n",
    "\n",
    "# predictions of binary vectors [0.43 0.93 0.07 0.66 0.42 0.89 0.00]\n",
    "# transformed to vectors looking like binary representation base od threshold [0 1 0 1 0 1 0]\n",
    "def predictions_to_binary(preds, threshold=0.5):\n",
    "    return np.where(preds > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset of words\n",
    "names = pd.read_csv(config['file']).drop_duplicates()\n",
    "names_list = list(names.values.reshape(-1))\n",
    "\n",
    "# transform words to sequence of letters and add termination character\n",
    "names_list = [[l for l in word+'|'] for word in names_list]\n",
    "\n",
    "# do the basic preprocessing (tokens mapping)\n",
    "prepared_data = sequences_prepared_data(names_list)\n",
    "\n",
    "# use the mappings to encode the sequences of tokens to sequences of integer identifiers\n",
    "prepared_data['encoded_tokens'] = encode_tokens_to_idx(\n",
    "    prepared_data['sequences_tokens_np'],\n",
    "    prepared_data['tokens_to_idx']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset preparation\n",
    "# this gets random subsample from the sequence of given length as inputs (x)\n",
    "# and following character as output (y)\n",
    "def get_chunk_and_next(token, token_size, chunk_size):\n",
    "    chunk_start_index = random.randint(0, token_size-chunk_size-1-1)\n",
    "    chunk = token[chunk_start_index:chunk_start_index+chunk_size]\n",
    "    next_char = token[chunk_start_index+chunk_size]\n",
    "    return chunk, next_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling of subsamples from sequences\n",
    "\n",
    "chunks = []\n",
    "next_chars = []\n",
    "\n",
    "# first part gets 2 chunks of given length from given sequence\n",
    "chunks_per_word = 2\n",
    "\n",
    "for token in prepared_data['encoded_tokens']:\n",
    "    for chunk in range(chunks_per_word):\n",
    "        chunk, next_char = get_chunk_and_next(token, token_size=prepared_data['max_tokens'], chunk_size=config['maxlen'])\n",
    "        chunks.append(chunk)\n",
    "        next_chars.append(next_char)\n",
    "\n",
    "# second part chops out the end of the sequence, as it is padded with empty strings\n",
    "# this leads to significant amount of empty chunks (x) and even more empty following chars (y)\n",
    "# this helps to reduce it\n",
    "# second part gets 2 chunks of given length from given sequence\n",
    "chunks_per_word = 3\n",
    "chop_chars = 4\n",
    "\n",
    "for token in prepared_data['encoded_tokens'][:,0:prepared_data['max_tokens']-chop_chars]:\n",
    "    for chunk in range(chunks_per_word):\n",
    "        chunk, next_char = get_chunk_and_next(token, token_size=prepared_data['max_tokens']-chop_chars, chunk_size=config['maxlen'])\n",
    "        chunks.append(chunk)\n",
    "        next_chars.append(next_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many bits are necessary for encoding such amount of unique tokens\n",
    "num_bits = math.ceil(math.sqrt(prepared_data['vocab_size']))\n",
    "\n",
    "#encode the input categories to binary encoding\n",
    "x = binary_system_encode(np.array(chunks), num_bits=num_bits)\n",
    "#encode the input categories to binary encoding\n",
    "next_chars_np = np.array(next_chars).reshape(-1,1)\n",
    "y = binary_system_encode(next_chars_np, num_bits=num_bits).reshape(next_chars_np.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error metric for evaluating prediction of binary encoded vector\n",
    "def custom_binary_error(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.bool)\n",
    "    y_pred = tf.cast(y_pred, tf.bool)\n",
    "    xored = tf.math.logical_xor(y_true, y_pred)\n",
    "    notxored = tf.math.logical_not(xored)\n",
    "    sum_xored = tf.reduce_sum(tf.cast(xored, tf.float32))\n",
    "    sum_notxored = tf.reduce_sum(tf.cast(notxored, tf.float32))\n",
    "    return sum_xored / (sum_xored + sum_notxored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "model = Sequential()\n",
    "# RNN/GRU/LSTM layer for sequence characteristic processing\n",
    "model.add(GRU(32, input_shape=(config['maxlen'], num_bits)))\n",
    "#model.add(Dense(num_bits, activation='sigmoid'))\n",
    "# dense layer of binary encoded vector using hard_sigmoid activation as we want highly polarized output\n",
    "# aka as much as close to either 0 or 1\n",
    "model.add(Dense(num_bits, activation='hard_sigmoid'))\n",
    "# loss function is binary_crossentropy, because in simplified scenario we are judging each correctly/falsely predicted bit separately\n",
    "# custom loss function addressing this issue can significantly improve the performance\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[custom_binary_error])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(x, y, batch_size=config['batch_size'], epochs=config['epochs'], callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['custom_binary_error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generating the whole sequence until terminating character is output\n",
    "def predict_word(seed, threshold, model, input_size, tokens_to_idx, idx_to_tokens, vocab_size):\n",
    "    predicted_word = seed\n",
    "    pred_char = ''\n",
    "    \n",
    "    num_bits = math.ceil(math.sqrt(vocab_size))\n",
    "    \n",
    "    for x in range(30):\n",
    "    \n",
    "        encoded_word = binary_system_encode(\n",
    "            encode_tokens_to_idx(\n",
    "                # as a sideeffect thanks to this slicing it will not throw an error when inputing seed longer than input size of the model\n",
    "                np.array([[x for x in predicted_word[-input_size:]],]),\n",
    "                tokens_to_idx\n",
    "            ),\n",
    "            num_bits=num_bits\n",
    "        )\n",
    "        # predict next charater predictions for binary encoding\n",
    "        preds = model.predict(encoded_word, verbose=0)[0]\n",
    "        # debug showing predicted values for the bits\n",
    "        print('preds: ', preds)\n",
    "        # plot to see the polarization and differences between values for specific bits\n",
    "        plt.bar(list(range(len(preds))), preds)\n",
    "        plt.show()\n",
    "        # convert the values to \"valid\" binary vector\n",
    "        # this can easily end with KeyError,\n",
    "        # because conversion of predicted output to binary vector using cutoff threshold doesn't guarantee to output integer in vocabulary\n",
    "        pred_char = idx_to_tokens[\n",
    "            binary_system_decode(\n",
    "                predictions_to_binary(preds, threshold=threshold)\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print('pred_char: ', pred_char)\n",
    "\n",
    "        predicted_word += pred_char\n",
    "        \n",
    "        if (pred_char == '|'):\n",
    "            break\n",
    "    \n",
    "    return predicted_word[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_word(\n",
    "    'Jam',\n",
    "    0.5,\n",
    "    model,\n",
    "    config['maxlen'],\n",
    "    prepared_data['tokens_to_idx'],\n",
    "    prepared_data['idx_to_tokens'],\n",
    "    prepared_data['vocab_size']\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
